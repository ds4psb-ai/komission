"""
Phase 3: LLM-Based Adaptive Coaching Service

ì‚¬ìš©ì í”¼ë“œë°±ì„ LLMìœ¼ë¡œ íŒŒì‹±í•˜ê³  DNAInvariantë¥¼ ê²€ì¦í•˜ì—¬
ë°”ì´ëŸ´ ìš”ì†Œë¥¼ ì†ìƒì‹œí‚¤ì§€ ì•ŠëŠ” ë²”ìœ„ì—ì„œ ëŒ€ì•ˆ ì œì‹œ

í•µì‹¬ ì›ì¹™:
- LLM ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— DNAInvariant ëª©ë¡ ì£¼ì…
- critical/high priority invariant â†’ ê±°ì ˆ + ëŒ€ì•ˆ ì œì‹œ
- medium/low priority ë˜ëŠ” ë¯¸ì§€ì • â†’ í—ˆìš©
"""
from typing import Optional, List, Dict, Any, Literal
from dataclasses import dataclass
from app.schemas.director_pack import DirectorPack, DNAInvariant, MutationSlot
import logging
import json

logger = logging.getLogger(__name__)


@dataclass
class UserFeedback:
    """íŒŒì‹±ëœ ì‚¬ìš©ì í”¼ë“œë°±"""
    raw_text: str
    feedback_type: Literal["cannot_do", "alternative_idea", "question", "ok", "creative"]
    affected_domain: Optional[str] = None  # composition, timing, audio, etc.
    affected_rule_id: Optional[str] = None  # ì§ì ‘ ì—°ê´€ëœ ê·œì¹™ ID
    proposed_change: Optional[str] = None
    user_reason: Optional[str] = None  # ì‚¬ìš©ì ì˜ë„/ì´ìœ 
    confidence: float = 0.0


@dataclass
class AdaptiveResponse:
    """ì ì‘í˜• ì½”ì¹­ ì‘ë‹µ"""
    accepted: bool
    message: str
    alternative: Optional[str] = None
    reason: Optional[str] = None
    affected_rule_id: Optional[str] = None
    coaching_adjustment: Optional[str] = None  # í—ˆìš© ì‹œ ì½”ì¹­ ì¡°ì • ë‚´ìš©


# ==================
# SYSTEM PROMPT TEMPLATES
# ==================

FEEDBACK_PARSER_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì‹¤ì‹œê°„ ì´¬ì˜ ì½”ì¹­ ì‹œìŠ¤í…œì˜ í”¼ë“œë°± ë¶„ì„ê¸°ì…ë‹ˆë‹¤.

## ì—­í• 
ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

## í˜„ì¬ ì½”ì¹­ ê·œì¹™ (DNAInvariant)
{invariants_json}

## í—ˆìš© ê°€ëŠ¥í•œ ë³€í˜• (MutationSlot)
{slots_json}

## í”¼ë“œë°± íƒ€ì…
- **cannot_do**: ì‚¬ìš©ìê°€ í˜„ì¬ ì½”ì¹­ì„ ë”°ë¥¼ ìˆ˜ ì—†ëŠ” ìƒí™© (ì¥ë¹„, í™˜ê²½, ë¬¼ë¦¬ì  ì œì•½)
- **alternative_idea**: ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ì°½ì˜ì  ì•„ì´ë””ì–´ë¥¼ ì œì•ˆ
- **question**: ì½”ì¹­ì— ëŒ€í•œ ì§ˆë¬¸
- **ok**: ì½”ì¹­ ìˆ˜ë½
- **creative**: ì°½ì˜ì  í‘œí˜„/ì—°ì¶œ ì œì•ˆ (ë°”ì´ëŸ´ ìš”ì†Œì™€ ë¬´ê´€í•œ ìˆœìˆ˜ ì°½ì‘)

## ì¶œë ¥ í˜•ì‹ (JSON)
```json
{
  "feedback_type": "cannot_do|alternative_idea|question|ok|creative",
  "affected_domain": "composition|timing|lighting|audio|performance|null",
  "affected_rule_id": "ê·œì¹™ID|null",
  "proposed_change": "ì‚¬ìš©ìê°€ ì œì•ˆí•˜ëŠ” ëŒ€ì•ˆ|null",
  "user_reason": "ì‚¬ìš©ìì˜ ì´ìœ /ì˜ë„|null",
  "confidence": 0.0-1.0
}
```

## ì£¼ì˜ì‚¬í•­
1. affected_rule_idëŠ” ìœ„ ê·œì¹™ ëª©ë¡ì—ì„œ ì •í™•íˆ ë§¤ì¹­ë˜ëŠ” ê²ƒë§Œ ì‚¬ìš©
2. creative íƒ€ì…ì€ ë°”ì´ëŸ´ í•µì‹¬(critical/high) ê·œì¹™ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜ ì°½ì‘
3. ëª¨í˜¸í•œ ê²½ìš° questionìœ¼ë¡œ ë¶„ë¥˜

ì‚¬ìš©ì í”¼ë“œë°±ì„ ë¶„ì„í•˜ì„¸ìš”."""


COACHING_DECISION_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì‹¤ì‹œê°„ ì´¬ì˜ ì½”ì¹­ ì‹œìŠ¤í…œì˜ ì ì‘í˜• ì½”ì¹˜ì…ë‹ˆë‹¤.

## ì—­í• 
ì‚¬ìš©ìì˜ í”¼ë“œë°±ì— ëŒ€í•´ ë°”ì´ëŸ´ ìš”ì†Œë¥¼ ë³´í˜¸í•˜ë©´ì„œ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•©ë‹ˆë‹¤.

## í•µì‹¬ ì›ì¹™
1. **ë°”ì´ëŸ´ DNA ë³´í˜¸**: critical/high priority ê·œì¹™ì€ ì ˆëŒ€ ì–‘ë³´í•˜ì§€ ì•ŠìŒ
2. **ìœ ì—°í•œ ëŒ€ì‘**: medium/low priority ê·œì¹™ì€ ì‚¬ìš©ì ì œì•ˆ ìˆ˜ìš© ê°€ëŠ¥
3. **ì°½ì˜ì„± ì¡´ì¤‘**: ë°”ì´ëŸ´ ìš”ì†Œì™€ ë¬´ê´€í•œ ì°½ì‘ì€ ì ê·¹ ì§€ì§€

## í˜„ì¬ ì½”ì¹­ ê·œì¹™
{invariants_json}

## í—ˆìš© ê°€ëŠ¥í•œ ë³€í˜•
{slots_json}

## ì‚¬ìš©ì í”¼ë“œë°± ë¶„ì„ ê²°ê³¼
{feedback_json}

## ì‘ë‹µ í˜•ì‹ (JSON)
```json
{
  "accepted": true|false,
  "message": "ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ (ì¹œê·¼í•˜ê³  ì§§ê²Œ)",
  "alternative": "ê±°ì ˆ ì‹œ ëŒ€ì•ˆ ì œì•ˆ|null",
  "reason": "ê²°ì • ì´ìœ  (ë‚´ë¶€ìš©)",
  "coaching_adjustment": "í—ˆìš© ì‹œ ì½”ì¹­ ì¡°ì • ë‚´ìš©|null"
}
```

## ì‘ë‹µ í†¤
- ì¹œê·¼í•˜ê³  ì§€ì§€ì ì¸ í†¤
- ê±°ì ˆí•´ë„ ëŒ€ì•ˆ ì œì‹œë¡œ ê¸ì •ì  ë§ˆë¬´ë¦¬
- í—ˆìš© ì‹œ ì ê·¹ì  ê²©ë ¤

ê²°ì •ì„ ë‚´ë ¤ì£¼ì„¸ìš”."""


class AdaptiveCoachingService:
    """
    Phase 3: LLM ê¸°ë°˜ ì ì‘í˜• ì½”ì¹­ ì„œë¹„ìŠ¤
    
    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— DNAInvariant ëª©ë¡ì„ ì£¼ì…í•˜ì—¬
    LLMì´ ë°”ì´ëŸ´ ìš”ì†Œë¥¼ ì´í•´í•˜ê³  ìŠ¤ë§ˆíŠ¸í•˜ê²Œ íŒë‹¨
    """
    
    def __init__(
        self,
        director_pack: Optional[DirectorPack] = None,
        llm_client: Optional[Any] = None,  # Gemini/OpenAI client
        use_llm: bool = True,  # LLM ì‚¬ìš© ì—¬ë¶€ (í´ë°±ìš©)
    ):
        self._pack = director_pack
        self._llm_client = llm_client
        self._use_llm = use_llm
        self._invariants_map: Dict[str, DNAInvariant] = {}
        self._slots_map: Dict[str, MutationSlot] = {}
        
        if director_pack:
            self._build_maps(director_pack)
    
    def _build_maps(self, pack: DirectorPack) -> None:
        """DNAInvariantì™€ MutationSlot ë§µ êµ¬ì¶•"""
        for inv in pack.dna_invariants:
            self._invariants_map[inv.rule_id] = inv
        
        for slot in pack.mutation_slots:
            self._slots_map[slot.slot_id] = slot
    
    def update_pack(self, pack: DirectorPack) -> None:
        """DirectorPack ì—…ë°ì´íŠ¸"""
        self._pack = pack
        self._invariants_map.clear()
        self._slots_map.clear()
        self._build_maps(pack)
    
    def _invariants_to_json(self) -> str:
        """DNAInvariantë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš© JSONìœ¼ë¡œ ë³€í™˜"""
        items = []
        for inv in self._invariants_map.values():
            items.append({
                "rule_id": inv.rule_id,
                "domain": inv.domain,
                "priority": inv.priority,  # critical, high, medium, low
                "description": inv.check_hint or "",
                "tolerance": inv.tolerance,
            })
        return json.dumps(items, ensure_ascii=False, indent=2)
    
    def _slots_to_json(self) -> str:
        """MutationSlotì„ LLM í”„ë¡¬í”„íŠ¸ìš© JSONìœ¼ë¡œ ë³€í™˜"""
        items = []
        for slot in self._slots_map.values():
            items.append({
                "slot_id": slot.slot_id,
                "description": slot.description or "",
                "variants": slot.variants[:5] if slot.variants else [],
            })
        return json.dumps(items, ensure_ascii=False, indent=2)
    
    async def parse_user_feedback_llm(self, text: str) -> UserFeedback:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì í”¼ë“œë°± íŒŒì‹±
        """
        if not self._use_llm or not self._llm_client:
            return self._parse_user_feedback_fallback(text)
        
        system_prompt = FEEDBACK_PARSER_SYSTEM_PROMPT.format(
            invariants_json=self._invariants_to_json(),
            slots_json=self._slots_to_json(),
        )
        
        try:
            # Gemini API í˜¸ì¶œ
            response = await self._call_llm(
                system_prompt=system_prompt,
                user_message=f"ì‚¬ìš©ì í”¼ë“œë°±: {text}",
            )
            
            # JSON íŒŒì‹±
            result = self._extract_json(response)
            
            return UserFeedback(
                raw_text=text,
                feedback_type=result.get("feedback_type", "question"),
                affected_domain=result.get("affected_domain"),
                affected_rule_id=result.get("affected_rule_id"),
                proposed_change=result.get("proposed_change"),
                user_reason=result.get("user_reason"),
                confidence=result.get("confidence", 0.8),
            )
            
        except Exception as e:
            logger.warning(f"LLM parsing failed, using fallback: {e}")
            return self._parse_user_feedback_fallback(text)
    
    async def generate_response_llm(self, feedback: UserFeedback) -> AdaptiveResponse:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì ì‘í˜• ì‘ë‹µ ìƒì„±
        """
        if not self._use_llm or not self._llm_client:
            return self._generate_response_fallback(feedback)
        
        feedback_json = json.dumps({
            "feedback_type": feedback.feedback_type,
            "affected_domain": feedback.affected_domain,
            "affected_rule_id": feedback.affected_rule_id,
            "proposed_change": feedback.proposed_change,
            "user_reason": feedback.user_reason,
        }, ensure_ascii=False)
        
        system_prompt = COACHING_DECISION_SYSTEM_PROMPT.format(
            invariants_json=self._invariants_to_json(),
            slots_json=self._slots_to_json(),
            feedback_json=feedback_json,
        )
        
        try:
            response = await self._call_llm(
                system_prompt=system_prompt,
                user_message="ìœ„ í”¼ë“œë°±ì— ëŒ€í•œ ê²°ì •ì„ ë‚´ë ¤ì£¼ì„¸ìš”.",
            )
            
            result = self._extract_json(response)
            
            return AdaptiveResponse(
                accepted=result.get("accepted", True),
                message=result.get("message", "ë„¤, ì•Œê² ì–´ìš”!"),
                alternative=result.get("alternative"),
                reason=result.get("reason"),
                affected_rule_id=feedback.affected_rule_id,
                coaching_adjustment=result.get("coaching_adjustment"),
            )
            
        except Exception as e:
            logger.warning(f"LLM response failed, using fallback: {e}")
            return self._generate_response_fallback(feedback)
    
    async def _call_llm(self, system_prompt: str, user_message: str) -> str:
        """
        LLM API í˜¸ì¶œ (Gemini ë˜ëŠ” OpenAI)
        """
        if hasattr(self._llm_client, 'generate_content'):
            # Gemini
            response = await self._llm_client.generate_content_async(
                contents=[
                    {"role": "user", "parts": [{"text": f"{system_prompt}\n\n{user_message}"}]}
                ],
                generation_config={
                    "temperature": 0.3,
                    "max_output_tokens": 500,
                }
            )
            return response.text
        
        elif hasattr(self._llm_client, 'chat'):
            # OpenAI
            response = await self._llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message},
                ],
                temperature=0.3,
                max_tokens=500,
            )
            return response.choices[0].message.content
        
        raise ValueError("Unknown LLM client type")
    
    def _extract_json(self, text: str) -> dict:
        """LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
        import re
        
        # ```json ... ``` ë¸”ë¡ ì°¾ê¸°
        json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        
        # { ... } ì°¾ê¸°
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(0))
        
        return {}
    
    # ==================
    # FALLBACK (í‚¤ì›Œë“œ ê¸°ë°˜)
    # ==================
    
    CANNOT_DO_KEYWORDS = ["ëª»", "ì•ˆ ë¼", "ë¶ˆê°€ëŠ¥", "can't", "cannot", "impossible", "ì–´ë ¤ì›Œ"]
    ALTERNATIVE_KEYWORDS = ["ëŒ€ì‹ ", "ì´ê±° ì–´ë•Œ", "ë‹¤ë¥¸", "instead", "how about", "what if"]
    QUESTION_KEYWORDS = ["?", "ì–´ë–»ê²Œ", "ì™œ", "how", "why", "what"]
    OK_KEYWORDS = ["ì•Œê² ", "ë„¤", "ok", "yes", "ì¢‹ì•„", "í™•ì¸"]
    
    DOMAIN_KEYWORDS = {
        "composition": ["êµ¬ë„", "ì¤‘ì•™", "ìœ„ì¹˜", "center", "position", "frame"],
        "timing": ["íƒ€ì´ë°", "ì‹œê°„", "ë¹¨ë¦¬", "ëŠë¦¬", "timing", "fast", "slow"],
        "lighting": ["ì¡°ëª…", "ì—­ê´‘", "ë°", "ì–´ë‘", "light", "bright", "dark"],
        "audio": ["ì†Œë¦¬", "ìŒì„±", "ëª©ì†Œë¦¬", "audio", "voice", "sound"],
        "performance": ["í‘œì •", "ì—°ê¸°", "ë™ì‘", "expression", "action", "gesture"],
    }
    
    def _parse_user_feedback_fallback(self, text: str) -> UserFeedback:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± íŒŒì‹±"""
        text_lower = text.lower()
        
        feedback_type: Literal["cannot_do", "alternative_idea", "question", "ok", "creative"] = "question"
        confidence = 0.5
        
        if any(kw in text for kw in self.CANNOT_DO_KEYWORDS):
            feedback_type = "cannot_do"
            confidence = 0.7
        elif any(kw in text for kw in self.ALTERNATIVE_KEYWORDS):
            feedback_type = "alternative_idea"
            confidence = 0.6
        elif any(kw in text for kw in self.OK_KEYWORDS):
            feedback_type = "ok"
            confidence = 0.9
        elif any(kw in text for kw in self.QUESTION_KEYWORDS):
            feedback_type = "question"
            confidence = 0.5
        
        affected_domain = None
        for domain, keywords in self.DOMAIN_KEYWORDS.items():
            if any(kw in text_lower for kw in keywords):
                affected_domain = domain
                break
        
        # ë„ë©”ì¸ì—ì„œ ê·œì¹™ ID ì°¾ê¸°
        affected_rule_id = None
        if affected_domain:
            for inv in self._invariants_map.values():
                if inv.domain == affected_domain:
                    affected_rule_id = inv.rule_id
                    break
        
        return UserFeedback(
            raw_text=text,
            feedback_type=feedback_type,
            affected_domain=affected_domain,
            affected_rule_id=affected_rule_id,
            proposed_change=None,
            user_reason=None,
            confidence=confidence,
        )
    
    def _generate_response_fallback(self, feedback: UserFeedback) -> AdaptiveResponse:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ì‘ë‹µ"""
        if feedback.feedback_type == "ok":
            return AdaptiveResponse(accepted=True, message="ì¢‹ì•„ìš”! ê³„ì† ì§„í–‰í•´ìš”!")
        
        if feedback.feedback_type == "question":
            return AdaptiveResponse(accepted=True, message="ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œêµ°ìš”!")
        
        # cannot_do / alternative_idea â†’ DNAInvariant ê²€ì¦
        affected_inv = self._invariants_map.get(feedback.affected_rule_id) if feedback.affected_rule_id else None
        
        if not affected_inv and feedback.affected_domain:
            for inv in self._invariants_map.values():
                if inv.domain == feedback.affected_domain:
                    affected_inv = inv
                    break
        
        if not affected_inv:
            return AdaptiveResponse(
                accepted=True,
                message="ì¢‹ì•„ìš”! ê·¸ë ‡ê²Œ í•´ë³¼ê¹Œìš”?",
            )
        
        if affected_inv.priority in ["critical", "high"]:
            return AdaptiveResponse(
                accepted=False,
                message=f"'{affected_inv.check_hint}'ì€(ëŠ”) ë°”ì´ëŸ´ í•µì‹¬ì´ì—ìš”.",
                alternative=self._get_alternative(feedback.affected_domain),
                reason=f"priority={affected_inv.priority}",
                affected_rule_id=affected_inv.rule_id,
            )
        
        return AdaptiveResponse(
            accepted=True,
            message="ì¢‹ì€ ì•„ì´ë””ì–´ë„¤ìš”! ì§„í–‰í•´ë³¼ê¹Œìš”?",
            affected_rule_id=affected_inv.rule_id,
        )
    
    def _get_alternative(self, domain: Optional[str]) -> str:
        """ë„ë©”ì¸ë³„ ëŒ€ì•ˆ ì œì‹œ"""
        ALTERNATIVES = {
            "composition": "ì‚¼ë¶„í•  êµ¬ë„ë‚˜ ì˜¤í”„ì„¼í„°ë„ ê´œì°®ì•„ìš”!",
            "timing": "ì¡°ê¸ˆ ì—¬ìœ ë¡­ê²Œ í•´ë„ ë¼ìš”!",
            "lighting": "ì¸¡ë©´ê´‘ì´ë‚˜ ìì—°ê´‘ë„ ì¢‹ì•„ìš”!",
            "audio": "ìë§‰ìœ¼ë¡œ ë³´ì™„í•  ìˆ˜ ìˆì–´ìš”!",
            "performance": "ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ì‘ë„ ì¢‹ì•„ìš”!",
        }
        return ALTERNATIVES.get(domain or "", "ë‹¤ë¥¸ ë°©ë²•ë„ ìˆì–´ìš”!")
    
    # ==================
    # PUBLIC API
    # ==================
    
    async def process_feedback(self, text: str) -> AdaptiveResponse:
        """
        ë©”ì¸ API: í”¼ë“œë°± ì²˜ë¦¬ (LLM ìš°ì„ , í´ë°± ì§€ì›)
        """
        logger.info(f"ğŸ¤ Processing feedback: {text[:50]}...")
        
        # 1. í”¼ë“œë°± íŒŒì‹±
        feedback = await self.parse_user_feedback_llm(text)
        logger.info(f"   â†’ type={feedback.feedback_type}, domain={feedback.affected_domain}, rule={feedback.affected_rule_id}")
        
        # 2. ì‘ë‹µ ìƒì„±
        response = await self.generate_response_llm(feedback)
        logger.info(f"   â†’ accepted={response.accepted}, msg={response.message[:30]}")
        
        return response
    
    def process_feedback_sync(self, text: str) -> AdaptiveResponse:
        """
        ë™ê¸° API: LLM ì—†ì´ í´ë°±ë§Œ ì‚¬ìš©
        """
        feedback = self._parse_user_feedback_fallback(text)
        return self._generate_response_fallback(feedback)
