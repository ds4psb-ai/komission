"""
Gemini Pipeline - VDG (Video Data Graph) v3.0 (PEGL v1.0)
Uses Gemini 3.0 Pro to extract the 'Brain' of the viral video.

PEGL v1.0: Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Ï†ÅÏö©
- Ï∂úÎ†• Ïä§ÌÇ§Îßà Î≤ÑÏ†Ñ Í≤ÄÏ¶ù
- ÌïÑÏàò ÌïÑÎìú Í≤ÄÏ¶ù
"""
import os
import json
import logging
import time
from typing import Optional, Dict, Any, List
from google import genai
from google.genai import types
from app.config import settings
from app.schemas.vdg import VDG, Scene, Shot, Keyframe, CapsuleBrief, ShotlistItem
from app.services.video_downloader import video_downloader
from app.validators.schema_validator import validate_vdg_analysis_schema, SchemaValidationError

logger = logging.getLogger(__name__)

VDG_PROMPT = """
ÎãπÏã†ÏùÄ Î∞îÏù¥Îü¥ ÏòÅÏÉÅ Ï†ÑÎ¨∏Í∞Ä AI (Gemini 3.0 Pro)ÏûÖÎãàÎã§.
Ïù¥ Î∞îÏù¥Îü¥ ÏòÅÏÉÅÏùÑ **VDG (Video Data Graph) v3.3** ÌòïÏãùÏúºÎ°ú Î∂ÑÏÑùÌïòÏÑ∏Ïöî.

## Ï§ëÏöî: ÌïúÍ∏ÄÎ°ú Î∂ÑÏÑù
- Î™®Îì† ÌÖçÏä§Ìä∏ Ï∂úÎ†•ÏùÄ **ÌïúÍ∏Ä**Î°ú ÏûëÏÑ±
- summary, dialogue, location, hook_script Îì± Î™®Îëê ÌïúÍ∏Ä
- ÏòÅÏñ¥ Î≤àÏó≠ ÌïÑÎìúÎäî ÏÉùÎûµ

## ANALYSIS GOALS

### 1. HOOK ANALYSIS (0-5Ï¥à)
- **Microbeats**: ÌõÖÏùÑ start‚Üíbuild‚Üípunch ÎπÑÌä∏Î°ú Î∂ÑÌï¥
- **Virality Analysis**: Ïôú Ïä§ÌÅ¨Î°§ÏùÑ Î©àÏ∂îÍ≤å ÌïòÎäîÍ∞Ä?
  - curiosity_gap: Ïñ¥Îñ§ Í∂ÅÍ∏àÏ¶ùÏùÑ Ïú†Î∞úÌïòÎäîÍ∞Ä?
  - meme_potential: remixable_action | catchphrase | reaction_face | dance | low
  - engagement_pattern: watch_in_loop | share_trigger | comment_bait | scroll_stop

### 2. NARRATIVE STRUCTURE (scenes)
Í∞Å Ïî¨ÏóêÏÑú ÏãùÎ≥Ñ:
- **Dialogue**: Ï†ÑÏ≤¥ ÎåÄÏÇ¨ (ÏõêÎ≥∏ Ïñ∏Ïñ¥ Í∑∏ÎåÄÎ°ú, Ïô∏Íµ≠Ïñ¥Î©¥ ÌïúÍ∏Ä Î≤àÏó≠ Ìè¨Ìï®)
- **Rhetoric**: sarcasm, rhetorical_question, ad_hominem, hyperbole, irony
- **Comedic Device**: expectation_subversion, anticlimax, juxtaposition, callback, slapstick
- **Shots**: Ïπ¥Î©îÎùº ÏõåÌÅ¨ (ÏÉ∑ ÌÉÄÏûÖ, ÏïµÍ∏Ä, Î¨¥Î∏åÎ®ºÌä∏)

### 3. FOCUS WINDOWS (RLÏö©)
ÏãúÏ≤≠Ïûê Ï£ºÏùò ÏßëÏ§ë Íµ¨Í∞Ñ 3-5Í∞ú:
- **Hotspot scores**: hook (0-1), interest (0-1), boundary (0-1)
- **Mise-en-sc√®ne**: composition, lighting, lens, camera_move
- **Entities**: Ï∫êÎ¶≠ÌÑ∞/Ïò§Î∏åÏ†ùÌä∏ (pose, emotion, outfit)
- **Tags**: narrative_roles (SETUP, TURN, REVEAL, PUNCHLINE)

### 4. CROSS-SCENE ANALYSIS (Ìå®ÌÑ¥ Ìï©ÏÑ±)
- **Consistent elements**: Ïî¨ Í∞Ñ ÏùºÍ¥ÄÎêú ÏöîÏÜå
- **Evolving elements**: Î≥ÄÌôîÌïòÎäî ÏöîÏÜå
- **Director intent**: Ïó∞Ï∂ú ÏùòÎèÑ

### 5. SENTIMENT ARC
Í∞êÏ†ï Î≥ÄÌôî Ï∂îÏ†Å:
- start_sentiment, end_sentiment, trajectory

### 6. ASR/OCR EXTRACTION
- **asr_transcript**: ÏùåÏÑ± Ï†ÑÏÇ¨ (ÌïúÍ∏ÄÎ°ú)
- **ocr_text**: ÌôîÎ©¥ ÌÖçÏä§Ìä∏ (ÌïúÍ∏ÄÎ°ú)

### 7. PSYCHOLOGICAL AUDIT
- **Irony Analysis**: Í∏∞ÎåÄ vs ÌòÑÏã§ Í∞≠
- **Dopamine Radar**: 0-10 Ï†êÏàò

### 8. PRODUCT PLACEMENT (O2O)
Ï†úÌíà/Î∏åÎûúÎìú Îì±Ïû• Ïãú:
- product_mentions: Ïù¥Î¶Ñ, Î∏åÎûúÎìú, Ïπ¥ÌÖåÍ≥†Î¶¨
- cta_types: link_bio, swipe_up, discount_code

### 9. AUDIENCE REACTION
- viral_signal: Î∞îÏù¥Îü¥ ÌïµÏã¨ Ïù¥Ïú† (ÌïúÍ∏Ä)
- overall_sentiment: positive/negative/mixed

### 10. REPLICATION GUIDE (capsule_brief)
- hook_script: ÌõÖ Ïû¨ÌòÑ Î∞©Î≤ï (ÌïúÍ∏Ä)
- shotlist: [{seq, duration, action, shot}]
- do_not: ÌïòÏßÄ ÎßêÏïÑÏïº Ìï† Í≤ÉÎì§ (ÌïúÍ∏Ä)

### 11. REMIX SUGGESTIONS (Î≥ÄÏ£º Ï†úÏïà) - ÌïÑÏàò 2Í∞ú Ïù¥ÏÉÅ
Í∞Å Î≥ÄÏ£º Ï†úÏïàÏóêÎäî:
- target_niche: Ïñ¥Îñ§ ÌÅ¨Î¶¨ÏóêÏù¥ÌÑ∞Í∞Ä ÌôúÏö©Ìï† Ïàò ÏûàÎäîÍ∞Ä (Ïòà: "Î∑∞Ìã∞ Î¶¨Î∑∞Ïñ¥", "Î®πÎ∞© ÌÅ¨Î¶¨ÏóêÏù¥ÌÑ∞")
- concept: Î≥ÄÏ£º Ïª®ÏÖâ (Ïòà: "Ïù¥ Ìè¨Îß∑Ïóê ÌôîÏû•Ìíà Î¶¨Î∑∞Î•º ÏûÖÌûàÎ©¥...")
- template_type: re_enact | mashup | parody | product_placement
- viral_element_to_keep: Î∞òÎìúÏãú Ïú†ÏßÄÌï¥Ïïº Ìï† Î∞îÏù¥Îü¥ ÏöîÏÜå
- variable_elements: Î≥ÄÍ≤Ω Í∞ÄÎä•Ìïú ÏöîÏÜåÎì§

### 12. PRODUCT PLACEMENT GUIDE (Ï≤¥ÌóòÎã® Î≥ÄÏ£ºÏö©)
Ï†úÌíà/Î∏åÎûúÎìúÎ•º ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏÇΩÏûÖÌïòÎ†§Î©¥:
- recommended_timing: Ï†úÌíà Îì±Ïû• Ï∂îÏ≤ú ÏãúÏ†ê (Ïòà: "Ï§ëÎ∞ò 3-5Ï¥à")
- invariant_elements: Î∞òÎìúÏãú Ïú†ÏßÄÌï† ÏöîÏÜå (Hook Íµ¨Ï°∞ Îì±)
- variable_elements: Î≥ÄÏ£º Í∞ÄÎä•Ìïú ÏöîÏÜå (ÏÜåÏû¨, Ïù∏Î¨º Îì±)
- product_slot: Ï†úÌíà ÏÇΩÏûÖ ÏúÑÏπò (Ïòà: "ÏÜåÌíà ÏûêÎ¶¨Ïóê Ï†úÌíà ÎåÄÏ≤¥")

## OUTPUT SCHEMA (JSON Only)
{
  "content_id": "video_id",
  "platform": "youtube|tiktok|instagram",
  "title": "Ï∂îÎ°†Îêú Ï†úÎ™© (ÌïúÍ∏Ä)",
  "duration_sec": 8.0,
  "upload_date": null,
  "summary": "2Î¨∏Ïû• ÌïúÍ∏Ä ÏöîÏïΩ",
  
  "metrics": {
    "view_count": 0, "like_count": 0, "comment_count": 0,
    "hashtags": ["#funny", "#viral"]
  },
  
  "hook_genome": {
    "start_sec": 0.0, "end_sec": 3.0,
    "pattern": "subversion|problem_solution|question|pattern_break",
    "delivery": "dialogue|visual_gag|voiceover",
    "strength": 0.85,
    "hook_summary": "Ìïú Î¨∏Ïû• ÏÑ§Î™Ö (ÌïúÍ∏Ä)",
    "microbeats": [
      {"t": 0.5, "role": "start", "cue": "audio", "note": "ÏÜêÎãòÏù¥ Ï†ïÏ§ëÌïòÍ≤å ÏßàÎ¨∏"},
      {"t": 2.1, "role": "build", "cue": "audio", "note": "ÏÇ¨Ïû•Ïù¥ ÎπÑÍº¨Îäî ÎãµÎ≥Ä"},
      {"t": 4.2, "role": "punch", "cue": "audio", "note": "ÏßÅÏ†ëÏ†ÅÏù∏ ÏöïÏÑ§ Ìà¨Ï≤ô"}
    ],
    "virality_analysis": {
      "curiosity_gap": "ÏÜêÎãòÏù¥ Ïñ¥ÎñªÍ≤å Î∞òÏùëÌï†Íπå?",
      "meme_potential": "remixable_action",
      "relatability_factor": "surprise_reveal",
      "engagement_pattern": "watch_in_loop"
    },
    "information_density": "low"
  },
  
  "scenes": [{
    "scene_id": "S01",
    "time_start": 0.0, "time_end": 8.0, "duration_sec": 8.0,
    "narrative_unit": {
      "role": "Hook",
      "summary": "ÌïúÍ∏Ä ÏöîÏïΩ",
      "dialogue": "ÎåÄÏÇ¨ ÏõêÎ≥∏ (Ïô∏Íµ≠Ïñ¥Î©¥ ÌïúÍ∏Ä Î≤àÏó≠)",
      "dialogue_lang": "ko",
      "rhetoric": ["sarcasm", "rhetorical_question"],
      "comedic_device": ["expectation_subversion", "anticlimax"]
    },
    "setting": {
      "location": "ÏãùÎãπ",
      "visual_style": {"lighting": "Natural", "edit_pace": "slow"},
      "audio_style": {"audio_events": []}
    },
    "shots": [{"shot_id": "S01_01", "start": 0.0, "end": 8.0, "camera": {"shot": "MS", "angle": "eye", "move": "static"}}]
  }],
  
  "focus_windows": [
    {
      "window_id": "W00",
      "t_window": [0, 3.5],
      "hotspot": {
        "reasons": ["hook", "cv_change"],
        "scores": {"hook": 0.9, "interest": 0.8, "boundary": 0.6},
        "confidence": 0.9
      },
      "mise_en_scene": {
        "composition": {"grid": "center", "subject_size": "CU"},
        "lighting": {"type": "soft_light"},
        "lens": {"fov_class": "medium", "dof": "shallow"},
        "camera_move": "static"
      },
      "entities": [
        {"label": "Ï£ºÏù∏Í≥µ", "traits": {"pose": "ÏïâÏïÑÏûàÏùå", "emotion": "Î¨¥ÌëúÏ†ï"}, "role_in_window": "SUBJECT"}
      ],
      "parent_scene_id": "S01",
      "tags": {"narrative_roles": ["SETUP"], "cinematic": ["STATIC_SHOT", "CLOSE_UP"]}
    }
  ],
  
  "cross_scene_analysis": {
    "global_summary": "ÏÖãÏóÖÎ∂ÄÌÑ∞ ÌéÄÏπòÎùºÏù∏ÍπåÏßÄ ÏôÑÍ≤∞Îêú ÏÑúÏÇ¨ Íµ¨Ï°∞ (ÌïúÍ∏Ä)",
    "consistent_elements": [
      {"aspect": "composition", "evidence": "Ï§ëÏïô ÌîÑÎ†àÏù¥Î∞ç Ïú†ÏßÄ", "scenes": ["S01"]}
    ],
    "evolving_elements": [
      {"dimension": "emotion_arc", "description": "Î¨¥ÌëúÏ†ï ‚Üí Í∏¥Ïû• ‚Üí ÏõÉÏùå", "evidence": "ÌëúÏ†ï Î≥ÄÌôî", "pattern": "escalating"}
    ],
    "director_intent": [
      {"technique": "slow_long_take", "intended_effect": "comedic_timing", "rationale": "ÎåÄÏÇ¨Ïóê ÏßëÏ§ë", "evidence": {"scenes": ["S01"], "cues": ["no cuts"]}}
    ],
    "entity_state_changes": [
      {"entity_id": "ÏÜêÎãò", "initial_state": "Ï†ïÏ§ëÌïú ÏÜêÎãò", "final_state": "ÎãπÎãπÌïú ÏÜåÎπÑÏûê", "triggering_event": "ÏÇ¨Ïû•Ïùò ÏöïÏÑ§", "scene_id": "S01", "time_span": [4.2, 7.8]}
    ]
  },
  
  "asr_transcript": {
    "lang": "ko",
    "transcript": "ÏùåÏÑ± Ï†ÑÏÇ¨ (ÌïúÍ∏Ä)"
  },
  
  "ocr_text": [
    {"text": "ÏûêÎßâ ÌÖçÏä§Ìä∏", "lang": "ko", "timestamp": 2.5}
  ],
  
  "intent_layer": {
    "hook_trigger": "shock",
    "hook_trigger_reason": "ÌïúÍ∏Ä ÏÑ§Î™Ö",
    "retention_strategy": "ÌïúÍ∏Ä ÏÑ§Î™Ö",
    "irony_analysis": {"setup": "ÌïúÍ∏Ä", "twist": "ÌïúÍ∏Ä", "gap_type": "expectation_subversion"},
    "dopamine_radar": {"visual_spectacle": 3, "audio_stimulation": 5, "narrative_intrigue": 8, "emotional_resonance": 6, "comedy_shock": 10},
    "sentiment_arc": {
      "start_sentiment": "neutral",
      "end_sentiment": "amused",
      "micro_shifts": [
        {"t": 2.1, "from_emotion": "neutral", "to_emotion": "tense", "cue": "ÎπÑÍº¨Îäî ÎãµÎ≥Ä"}
      ],
      "trajectory": "Í∏¥Ïû•Í∞ê ÏÉÅÏäπ ÌõÑ Ïú†Î®∏Î°ú Ï†ÑÌôò"
    }
  },
  
  "commerce": {
    "product_mentions": [],
    "service_mentions": [],
    "cta_types": [],
    "has_sponsored_content": false
  },
  
  "remix_suggestions": [
    {
      "target_niche": "Î∑∞Ìã∞ Î¶¨Î∑∞Ïñ¥",
      "concept": "Ïù¥ Î¶¨Ïï°ÏÖò Ìè¨Îß∑Ïóê ÌôîÏû•Ìíà ÏÇ¨Ïö© Ï†ÑÌõÑ ÎπÑÍµêÎ•º ÏûÖÌûàÎ©¥ ÏûêÏó∞Ïä§ÎüΩÍ≤å Î∞îÏù¥Îü¥ Í∞ÄÎä•",
      "template_type": "product_placement",
      "viral_element_to_keep": "Î¨¥ÌëúÏ†ï ‚Üí ÎÜÄÎûå ‚Üí ÎßåÏ°± Í∞êÏ†ï Î≥ÄÌôî Íµ¨Ï°∞",
      "variable_elements": ["ÏÜåÏû¨Î•º Î∑∞Ìã∞ Ï†úÌíàÏúºÎ°ú ÍµêÏ≤¥", "Î∞∞Í≤ΩÏùÑ ÌôîÏû•ÎåÄÎ°ú Î≥ÄÍ≤Ω"]
    },
    {
      "target_niche": "Î®πÎ∞© ÌÅ¨Î¶¨ÏóêÏù¥ÌÑ∞",
      "concept": "ÏùåÏãù Î¶¨Î∑∞Ïóê Ïù¥ ÏÑúÌîÑÎùºÏù¥Ï¶à Ìè¨Îß∑ Ï†ÅÏö©",
      "template_type": "re_enact",
      "viral_element_to_keep": "3Ï¥à ÎÇ¥ Ìò∏Í∏∞Ïã¨ Ïú†Î∞ú Hook",
      "variable_elements": ["Ïù∏Î¨º ÍµêÏ≤¥", "ÏùåÏãùÏúºÎ°ú ÏÜåÏû¨ Î≥ÄÍ≤Ω"]
    }
  ],
  
  "capsule_brief": {
    "hook_script": "ÌõÖ Ïû¨ÌòÑ Î∞©Î≤ï (ÌïúÍ∏Ä)",
    "shotlist": [{"seq": 1, "duration": 3.0, "action": "ÌïúÍ∏Ä Ïï°ÏÖò", "shot": "MS"}],
    "constraints": {"min_actors": 2, "locations": ["ÏãùÎãπ"], "props": [], "difficulty": "Ïâ¨ÏõÄ", "primary_challenge": "ÏΩîÎØπ ÌÉÄÏù¥Î∞ç"},
    "do_not": ["Ï∫êÎ¶≠ÌÑ∞ Íπ®ÏßÄ Îßê Í≤É"],
    "product_placement_guide": {
      "recommended_timing": "Ï§ëÎ∞ò 3-5Ï¥à ÏÇ¨Ïù¥ ÏûêÏó∞Ïä§ÎüΩÍ≤å",
      "invariant_elements": ["Hook Íµ¨Ï°∞ (Ï≤òÏùå 3Ï¥à)", "Í∞êÏ†ï Î≥ÄÌôî Ìå®ÌÑ¥"],
      "variable_elements": ["ÏÜåÏû¨/Ï†úÌíà", "Ï¥¨ÏòÅ Ïû•ÏÜå", "Ïù∏Î¨º"],
      "product_slot": "ÏÜåÌíà ÏûêÎ¶¨Ïóê Ï†úÌíà ÎåÄÏ≤¥"
    }
  },
  
  "audience_reaction": {
    "analysis": "ÏãúÏ≤≠ÏûêÍ∞Ä Ïôú Ïù¥Î†áÍ≤å Î∞òÏùëÌñàÎäîÏßÄ Î∂ÑÏÑù (ÌïúÍ∏Ä)",
    "common_reactions": ["ÏõÉÏùå", "ÎπàÏ†ïÍ±∞Î¶º Í≥µÍ∞ê", "Í≥µÍ∞ê"],
    "overall_sentiment": "positive",
    "viral_signal": "Î∞îÏù¥Îü¥ ÌïµÏã¨ Ïù¥Ïú† Ìïú Ï§Ñ (ÌïúÍ∏Ä)"
  }
}
"""


class GeminiPipeline:
    def __init__(self):
        self.model = settings.GEMINI_MODEL
        # Prefer GEMINI_API_KEY first (GOOGLE_API_KEY was leaked)
        api_key = settings.GEMINI_API_KEY or settings.GOOGLE_API_KEY
        if api_key:
            if settings.GEMINI_API_KEY and settings.GOOGLE_API_KEY:
                logger.info("Using GEMINI_API_KEY (preferred)")
            self.client = genai.Client(api_key=api_key)
        else:
            self.client = None
            logger.warning("No API key set. GeminiPipeline will use mock data.")

    async def analyze_video(
        self, 
        video_url: str, 
        node_id: str,
        audience_comments: Optional[List[Dict[str, Any]]] = None
    ) -> VDG:
        """
        Full pipeline: Download -> Upload -> Analyze (VDG) -> Parse -> Return
        
        Args:
            video_url: Video URL to analyze
            node_id: Node ID for tracking
            audience_comments: Optional list of best comments for context
                [{"text": "...", "likes": 123, "lang": "en"}, ...]
        """
        if not self.client:
            logger.info("No API key, returning mock VDG data.")
            return self._get_mock_data(node_id)

        temp_path = None
        try:
            # 1. Download Video
            logger.info(f"üì• Downloading video from {video_url}...")
            temp_path, metadata = await video_downloader.download(video_url)
            
            # 2. Upload to Gemini
            logger.info(f"‚¨ÜÔ∏è Uploading for {node_id}...")
            video_file = self.client.files.upload(file=temp_path)
            
            # Wait for processing
            max_wait = 60
            elapsed = 0
            while elapsed < max_wait:
                file_info = self.client.files.get(name=video_file.name)
                if file_info.state.name == "ACTIVE":
                    logger.info(f"‚úÖ File ACTIVE: {video_file.name}")
                    break
                logger.info(f"‚è≥ Waiting... ({elapsed}s)")
                time.sleep(2)
                elapsed += 2
            else:
                raise Exception(f"File did not become ACTIVE within {max_wait}s")

            # 3. Build prompt with audience comments context
            enhanced_prompt = VDG_PROMPT
            if audience_comments:
                comments_text = "\n".join([
                    f"- [{c.get('likes', 0)} likes] {c.get('text', '')[:200]}"
                    for c in audience_comments[:10]
                ])
                enhanced_prompt = f"""
## AUDIENCE REACTIONS (Best Comments by Likes)
The following are the top comments from real viewers. Use these to understand WHY this video went viral:

{comments_text}

Consider these reactions when analyzing the hook effectiveness, emotional impact, and virality factors.

---

{VDG_PROMPT}
"""
                logger.info(f"üìù Including {len(audience_comments)} audience comments in analysis")

            # 4. Generate Analysis
            logger.info(f"üß† Analyzing {node_id} with {self.model} (VDG v3.0)...")
            
            try:
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=[video_file, enhanced_prompt],
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json"
                    )
                )
            except Exception as e:
                # Some environments expect fully-qualified model names (models/...)
                if "NOT_FOUND" in str(e) and not self.model.startswith("models/"):
                    fallback_model = f"models/{self.model}"
                    logger.warning(f"Model not found. Retrying with {fallback_model}")
                    response = self.client.models.generate_content(
                        model=fallback_model,
                        contents=[video_file, enhanced_prompt],
                        config=types.GenerateContentConfig(
                            response_mime_type="application/json"
                        )
                    )
                else:
                    raise


            # 4. Parse Response
            try:
                result_json = json.loads(response.text)
                result_json = self._sanitize_vdg_payload(result_json)
                result_json["content_id"] = node_id
                
                # PEGL v1.0: Ïä§ÌÇ§Îßà Î≤ÑÏ†Ñ Ï∂îÍ∞Ä (ÏóÜÏúºÎ©¥)
                if "schema_version" not in result_json:
                    result_json["schema_version"] = "v3.2"
                
                # PEGL v1.0: Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù (Ïã§Ìå® Ïãú Î™ÖÏãúÏ†Å ÏòàÏô∏)
                try:
                    validate_vdg_analysis_schema(result_json)
                except SchemaValidationError as e:
                    logger.error(f"Schema validation failed: {e}")
                    # Í≤ÄÏ¶ù Ïã§Ìå®Ìï¥ÎèÑ Í≥ÑÏÜç ÏßÑÌñâÌïòÎêò Í≤ΩÍ≥† Î°úÍ∑∏
                    # ÌîÑÎ°úÎçïÏÖòÏóêÏÑúÎäî raiseÎ°ú Î≥ÄÍ≤Ω Í∞ÄÎä•
                    logger.warning(f"Continuing despite schema validation failure for {node_id}")
                
                # Create VDG object
                vdg = VDG(**result_json)
                
                # === ADAPTER: Populate Legacy Fields ===
                self._populate_legacy_fields(vdg)
                
                # === VDG Quality Validation ===
                try:
                    from app.validators.vdg_quality_validator import validate_vdg_quality
                    quality_result = validate_vdg_quality(result_json)
                    
                    if quality_result.is_valid:
                        logger.info(f"‚úÖ VDG quality check PASSED (score: {quality_result.score})")
                    else:
                        logger.warning(
                            f"‚ö†Ô∏è VDG quality check FAILED (score: {quality_result.score})\n"
                            f"   Issues: {quality_result.issues[:3]}"
                        )
                    
                    # ÌíàÏßà Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï≤®Î∂Ä (result_jsonÏóê Ï∂îÍ∞Ä)
                    result_json["_quality_score"] = quality_result.score
                    result_json["_quality_valid"] = quality_result.is_valid
                    result_json["_quality_issues"] = quality_result.issues[:5]
                    
                except Exception as e:
                    logger.warning(f"VDG quality validation skipped: {e}")
                
                logger.info(f"‚úÖ VDG analysis complete for {node_id}")
                return vdg

            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse Gemini JSON response: {e}")
                logger.error(f"Raw response: {response.text[:500]}...")
                raise SchemaValidationError(f"Invalid JSON from Gemini: {e}", context="gemini_response")
            except Exception as e:
                logger.error(f"Failed to parse Gemini response: {e}")
                logger.error(f"Raw response: {response.text[:500]}...")
                raise e

        except Exception as e:
            logger.error(f"‚ùå Gemini analysis failed: {e}")
            raise e
        finally:
            if temp_path and os.path.exists(temp_path):
                os.remove(temp_path)

    def _sanitize_vdg_payload(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Coerce known schema edge-cases from LLM output before Pydantic validation."""
        scenes = payload.get("scenes", [])
        for scene in scenes:
            setting = scene.get("setting") or {}
            audio_style = setting.get("audio_style") or {}
            audio_events = audio_style.get("audio_events")
            if isinstance(audio_events, list):
                normalized = []
                for idx, event in enumerate(audio_events):
                    if isinstance(event, dict):
                        normalized.append(event)
                    elif isinstance(event, str):
                        normalized.append(
                            {
                                "timestamp": 0.0,
                                "event": "note",
                                "description": event,
                                "intensity": "medium",
                            }
                        )
                audio_style["audio_events"] = normalized
                setting["audio_style"] = audio_style
                scene["setting"] = setting

        commerce = payload.get("commerce") or {}
        service_mentions = commerce.get("service_mentions")
        if isinstance(service_mentions, list):
            normalized_services = []
            for item in service_mentions:
                if isinstance(item, str):
                    normalized_services.append(item)
                elif isinstance(item, dict):
                    name = item.get("name") or item.get("brand") or item.get("category")
                    normalized_services.append(name or str(item))
                else:
                    normalized_services.append(str(item))
            commerce["service_mentions"] = normalized_services
            payload["commerce"] = commerce

        # === Localization Check: Detect English-only fields ===
        # If scene summary is English-only, add summary_ko = None for frontend fallback
        for scene in scenes:
            nu = scene.get("narrative_unit", {})
            summary = nu.get("summary", "")
            if summary:
                # Check if summary contains Korean characters
                has_korean = any('\uac00' <= c <= '\ud7a3' for c in summary)
                if not has_korean:
                    # English-only summary detected - mark for frontend
                    nu["summary_ko"] = None
                    logger.warning(
                        f"Scene {scene.get('scene_id', '?')} has English-only summary: "
                        f"{summary[:50]}... (will show [EN] in UI)"
                    )
                else:
                    # Korean summary - copy to summary_ko for explicit handling
                    nu["summary_ko"] = summary
                scene["narrative_unit"] = nu

        return payload

    def _populate_legacy_fields(self, vdg: VDG) -> None:
        """Populate legacy compatibility fields for frontend"""
        # global_context
        hook = vdg.hook_genome
        intent = vdg.intent_layer
        
        vdg.global_context = {
            "title": vdg.title[:100] if vdg.title else "Analyzed Video",
            "mood": "dynamic", # simplified
            "keywords": ["viral", vdg.platform, intent.hook_trigger],
            "hashtags": [],
            "video_id": vdg.content_id,
            "hook_pattern": hook.pattern,
            "hook_delivery": hook.delivery,
            "hook_strength_score": hook.strength,
            "viral_hook_summary": hook.hook_summary,
            "key_action_description": intent.hook_trigger_reason
        }
        
        # scene_frames (simplified from scenes)
        frames = []
        for scene in vdg.scenes:
            for shot in scene.shots:
                frame = {
                    "timestamp": shot.start,
                    "duration": shot.end - shot.start,
                    "description": shot.keyframes[0].desc if shot.keyframes else scene.narrative_unit.summary,
                    "camera": {
                        "type": shot.camera.move,
                        "shot_size": shot.camera.shot,
                        "angle": shot.camera.angle
                    }
                }
                frames.append(frame)
        vdg.scene_frames = frames

    def _get_mock_data(self, video_id: str) -> VDG:
        """Return mock VDG data when API key is not available"""
        # (Mock implementation simplified for brevity)
        return VDG(
            content_id=video_id,
            title="Mock Video",
            duration_sec=15.0,
            summary="Mock summary",
            hook_genome=dict(hook_summary="Mock hook", strength=0.8),
            scenes=[],
            intent_layer=dict(
                hook_trigger="shock", 
                irony_analysis=dict(setup="A", twist="B"),
                dopamine_radar=dict(visual_spectacle=5, audio_stimulation=5, narrative_intrigue=5, emotional_resonance=5, comedy_shock=5)
            ),
            remix_suggestions=[],
            capsule_brief=dict(hook_script="Mock script", constraints=dict(primary_challenge="Mock challenge"))
        )

gemini_pipeline = GeminiPipeline()
